{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/anderson/Documents/word2vec'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/home/anderson/Documents/word2vec/\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_keys= pd.read_csv(\"data_qry.csv\", header=5, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_sub= pd.DataFrame(data_keys, columns=[\"Search term\",\"Match type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_sub.columns=[\"search_term\", \"match_type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_extract= data_sub[(data_sub['match_type']=='broad')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>search_term</th>\n",
       "      <th>match_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>victoria falls ausflugsmöglichkeiten</td>\n",
       "      <td>broad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>malaga concerts excursions etc en novembre 2014</td>\n",
       "      <td>broad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>welcome cardkeln</td>\n",
       "      <td>broad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>new york travel guide foot</td>\n",
       "      <td>broad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>museum tour wien</td>\n",
       "      <td>broad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       search_term match_type\n",
       "0             victoria falls ausflugsmöglichkeiten      broad\n",
       "1  malaga concerts excursions etc en novembre 2014      broad\n",
       "2                                 welcome cardkeln      broad\n",
       "3                       new york travel guide foot      broad\n",
       "4                                 museum tour wien      broad"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_extract=data_extract.reset_index(drop=True)\n",
    "data_extract.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_words(dirty_words):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    #\n",
    "    # 1. Remove square brackets\n",
    "    square_removal = BeautifulSoup(dirty_words).get_text()\n",
    "    #  \n",
    "    # 2. Remove non-letters\n",
    "    text_only = re.sub(\"[^a-zA-Z]\",\" \", square_removal)\n",
    "    #\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = text_only.lower().split()\n",
    "    #\n",
    "    # 4. Create list of words\n",
    "    word_list = [w for w in words]\n",
    "    #\n",
    "    # 5. Return a list of lists\n",
    "    return(word_list)\n",
    "\n",
    "def clean_phrases(dirty_words):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    #\n",
    "    # 1. Remove square brackets\n",
    "    square_removal = BeautifulSoup(dirty_words).get_text()\n",
    "    #  \n",
    "    # 2. Remove non-letters\n",
    "    text_only = re.sub(\"[^a-zA-Z]\",\"_\", square_removal)\n",
    "    #\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = text_only.lower().split()\n",
    "    #\n",
    "    # 4. Create list of words\n",
    "    word_list = [w for w in words]\n",
    "    #\n",
    "    # 5. Return a list of lists\n",
    "    return(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "victoria falls ausflugsmöglichkeiten\n"
     ]
    }
   ],
   "source": [
    "sub= data_extract.search_term[0]\n",
    "print sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the number of keyword entries based on the dataframe column size\n",
    "num_keywords = len(data_extract)\n",
    "# Initialize an empty list to hold the tokenized keywords\n",
    "tokenized_keys = []\n",
    "\n",
    "# Loop over each review; create an index i that goes from 0 to the length\n",
    "# of the keyword list \n",
    "for i in xrange(0, num_keywords):\n",
    "    # Call our function for each one, and add the result to the list of\n",
    "    # tokenized keywords\n",
    "    tokenized_keys.append( clean_words(data_extract.search_term[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'victoria', u'falls', u'ausflugsm', u'glichkeiten']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_keys[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s:%(levelname)s:%(messages)s', level= logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_features=300\n",
    "min_word_count=40\n",
    "num_workers=4\n",
    "context=10\n",
    "downsampling= 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model\n"
     ]
    }
   ],
   "source": [
    "print \"Training model\"\n",
    "model= word2vec.Word2Vec(tokenized_keys, workers=num_workers, size=num_features, min_count= min_word_count,\\\n",
    "window= context, sample= downsampling)\n",
    "model.init_sims(replace=True)\n",
    "model_name= \"300feature_search_terms_broad\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'gide', 0.6543757915496826),\n",
       " (u'guides', 0.6310587525367737),\n",
       " (u'find', 0.6148226261138916),\n",
       " (u'our', 0.5780949592590332),\n",
       " (u'guid', 0.5697128772735596),\n",
       " (u'your', 0.5556690096855164),\n",
       " (u'tourguide', 0.5499275326728821),\n",
       " (u'way', 0.54803466796875),\n",
       " (u'get', 0.5471200942993164),\n",
       " (u'russian', 0.5435190796852112)]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the number of keyword entries based on the dataframe column size\n",
    "num_keywords = len(data_extract)\n",
    "# Initialize an empty list to hold the tokenized keywords\n",
    "key_phrases= []\n",
    "\n",
    "# Loop over each review; create an index i that goes from 0 to the length\n",
    "# of the keyword list \n",
    "for i in xrange(0, num_keywords):\n",
    "    # Call our function for each one, and add the result to the list of\n",
    "    # tokenized keywords\n",
    "    key_phrases.append(clean_phrases(data_extract.search_term[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'victoria', u'falls', u'ausflugsm', u'glichkeiten']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_keys[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim as gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngrams= gs.models.phrases.Phrases(tokenized_keys,min_count=5,\\\n",
    "        threshold=10.0, delimiter='_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bigram_transformer = Phrases(key_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model= word2vec.Word2Vec.load(\"300feature_search_terms_broad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'welcome', 0.5177892446517944),\n",
       " (u'abc', 0.4719970226287842),\n",
       " (u'berliner', 0.47185343503952026),\n",
       " (u'alexanderplatz', 0.46284401416778564),\n",
       " (u'bvg', 0.4620282053947449),\n",
       " (u'discounts', 0.4544591009616852),\n",
       " (u'rabatte', 0.44964665174484253),\n",
       " (u'main', 0.4277723729610443),\n",
       " (u'lisbon', 0.42146673798561096),\n",
       " (u'hamburg', 0.4120604395866394)]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['germany', 'berlin'], negative=['france'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70569431527179838"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('paris','france')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bigram= Phrases(tokenized_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent = [u'hotel',u'london', u'tower', u'bridge', u'new', u'york', u'germany', u'berlin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'hotel', u'london', u'tower_bridge', u'new_york', u'germany', u'berlin']\n"
     ]
    }
   ],
   "source": [
    "print(bigram[sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram= Phrases(bigram[sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'hotel', u'london', u'tower_bridge', u'new_york', u'germany', u'berlin']\n"
     ]
    }
   ],
   "source": [
    "print(trigram[bigram[sent]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
